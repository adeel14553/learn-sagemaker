{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machines on MovieLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download ml-100k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ml-100k.zip\n",
      "   creating: ml-100k/\n",
      "  inflating: ml-100k/allbut.pl       \n",
      "  inflating: ml-100k/mku.sh          \n",
      "  inflating: ml-100k/README          \n",
      "  inflating: ml-100k/u.data          \n",
      "  inflating: ml-100k/u.genre         \n",
      "  inflating: ml-100k/u.info          \n",
      "  inflating: ml-100k/u.item          \n",
      "  inflating: ml-100k/u.occupation    \n",
      "  inflating: ml-100k/u.user          \n",
      "  inflating: ml-100k/u1.base         \n",
      "  inflating: ml-100k/u1.test         \n",
      "  inflating: ml-100k/u2.base         \n",
      "  inflating: ml-100k/u2.test         \n",
      "  inflating: ml-100k/u3.base         \n",
      "  inflating: ml-100k/u3.test         \n",
      "  inflating: ml-100k/u4.base         \n",
      "  inflating: ml-100k/u4.test         \n",
      "  inflating: ml-100k/u5.base         \n",
      "  inflating: ml-100k/u5.test         \n",
      "  inflating: ml-100k/ua.base         \n",
      "  inflating: ml-100k/ua.test         \n",
      "  inflating: ml-100k/ub.base         \n",
      "  inflating: ml-100k/ub.test         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2021-06-19 05:39:24--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4924029 (4.7M) [application/zip]\n",
      "Saving to: ‘ml-100k.zip’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1%  540K 9s\n",
      "    50K .......... .......... .......... .......... ..........  2% 1014K 7s\n",
      "   100K .......... .......... .......... .......... ..........  3% 98.3M 4s\n",
      "   150K .......... .......... .......... .......... ..........  4%  150M 3s\n",
      "   200K .......... .......... .......... .......... ..........  5% 1023K 3s\n",
      "   250K .......... .......... .......... .......... ..........  6% 90.5M 3s\n",
      "   300K .......... .......... .......... .......... ..........  7%  125M 2s\n",
      "   350K .......... .......... .......... .......... ..........  8%  186M 2s\n",
      "   400K .......... .......... .......... .......... ..........  9% 1.01M 2s\n",
      "   450K .......... .......... .......... .......... .......... 10%  216M 2s\n",
      "   500K .......... .......... .......... .......... .......... 11% 63.1M 2s\n",
      "   550K .......... .......... .......... .......... .......... 12%  111M 2s\n",
      "   600K .......... .......... .......... .......... .......... 13%  107M 2s\n",
      "   650K .......... .......... .......... .......... .......... 14%  194M 1s\n",
      "   700K .......... .......... .......... .......... .......... 15%  147M 1s\n",
      "   750K .......... .......... .......... .......... .......... 16%  132M 1s\n",
      "   800K .......... .......... .......... .......... .......... 17%  134M 1s\n",
      "   850K .......... .......... .......... .......... .......... 18% 1.05M 1s\n",
      "   900K .......... .......... .......... .......... .......... 19% 99.0M 1s\n",
      "   950K .......... .......... .......... .......... .......... 20%  300M 1s\n",
      "  1000K .......... .......... .......... .......... .......... 21% 68.9M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 22%  137M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 23%  106M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 24%  158M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 25%  160M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 27% 97.0M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 28%  152M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 29%  134M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 30%  147M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 31%  122M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 32% 1.09M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 33%  262M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 34% 66.3M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 35%  132M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 36% 65.8M 1s\n",
      "  1750K .......... .......... .......... .......... .......... 37%  150M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 38%  168M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 39%  113M 1s\n",
      "  1900K .......... .......... .......... .......... .......... 40%  121M 1s\n",
      "  1950K .......... .......... .......... .......... .......... 41%  140M 0s\n",
      "  2000K .......... .......... .......... .......... .......... 42% 79.2M 0s\n",
      "  2050K .......... .......... .......... .......... .......... 43%  333M 0s\n",
      "  2100K .......... .......... .......... .......... .......... 44%  170M 0s\n",
      "  2150K .......... .......... .......... .......... .......... 45%  206M 0s\n",
      "  2200K .......... .......... .......... .......... .......... 46% 84.2M 0s\n",
      "  2250K .......... .......... .......... .......... .......... 47%  171M 0s\n",
      "  2300K .......... .......... .......... .......... .......... 48%  139M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 49% 51.6M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 50%  261M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 51%  328M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 53%  243M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 54%  111M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 55%  151M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 56%  174M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 57%  160M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 58%  137M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 59% 28.5M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 60%  289M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 61%  394M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 62%  326M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 63% 1.29M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 64%  151M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 65%  197M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 66% 63.7M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 67% 66.1M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 68%  159M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 69%  178M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 70% 50.0M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 71%  111M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 72%  183M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 73% 68.8M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 74%  167M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 75%  148M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 76%  138M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 77% 68.0M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 79%  194M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 80%  144M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 81%  146M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 82% 78.0M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 83%  142M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 84%  160M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 85% 95.4M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 86%  183M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 87%  213M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 88%  107M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 89%  156M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 90% 96.4M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 91%  146M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 92%  133M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 93%  131M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 94%  183M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 95% 1.32M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 96% 55.9M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 97% 59.7M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 98%  339M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 99% 68.9M 0s\n",
      "  4800K ........                                              100%  148M=0.4s\n",
      "\n",
      "2021-06-19 05:39:25 (10.6 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "unzip -o ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Learn-Amazon-SageMaker/sdkv2/ch4/ml-100k\n",
      "894\t332\t3\t879896233\n",
      "413\t471\t4\t879969642\n",
      "276\t288\t4\t874786392\n",
      "450\t336\t3\t882370464\n",
      "151\t1006\t1\t879524974\n"
     ]
    }
   ],
   "source": [
    "%cd ml-100k\n",
    "!shuf ua.base -o ua.base.shuffled\n",
    "!head -5 ua.base.shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define sizing constants:\n",
    "num_users=943\n",
    "num_movies=1682\n",
    "num_features=num_users+num_movies\n",
    "\n",
    "num_ratings_train=90570\n",
    "num_ratings_test=9430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset into sparse matrix using custom fn\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def loadDataset(filename, lines, columns):\n",
    "    # Features are one-hot encoded in a sparse matrix\n",
    "    X = lil_matrix((lines, columns)).astype('float32')\n",
    "    # Labels are stored in a vector\n",
    "    Y = []\n",
    "    line=0\n",
    "    with open(filename,'r') as f:\n",
    "        samples=csv.reader(f,delimiter='\\t')\n",
    "        for userId,movieId,rating,timestamp in samples:\n",
    "            X[line,int(userId)-1] = 1\n",
    "            X[line,int(num_users)+int(movieId)-1] = 1\n",
    "            Y.append(int(rating))\n",
    "            line=line+1       \n",
    "    Y=np.array(Y).astype('float32')\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = loadDataset('ua.base.shuffled', num_ratings_train, num_features)\n",
    "X_test, Y_test = loadDataset('ua.test', num_ratings_test, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90570, 2625)\n",
      "(90570,)\n",
      "(9430, 2625)\n",
      "(9430,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "assert X_train.shape == (num_ratings_train, num_features)\n",
    "assert Y_train.shape == (num_ratings_train, )\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "assert X_test.shape  == (num_ratings_test, num_features)\n",
    "assert Y_test.shape  == (num_ratings_test, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to protobuf and save to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'adeelml-fm-movielens'\n",
    "\n",
    "train_key      = 'train.protobuf'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "\n",
    "test_key       = 'test.protobuf'\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test')\n",
    "\n",
    "output_prefix  = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BytesIO object at 0x7fbc31dd5170>\n",
      "<_io.BytesIO object at 0x7fbc3828e350>\n",
      "s3://sagemaker-us-west-1-886035371869/adeelml-fm-movielens/train/train.protobuf\n",
      "s3://sagemaker-us-west-1-886035371869/adeelml-fm-movielens/test/test.protobuf\n",
      "Output: s3://sagemaker-us-west-1-886035371869/adeelml-fm-movielens/output\n"
     ]
    }
   ],
   "source": [
    "import io, boto3\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "def writeDatasetToProtobuf(X, Y, bucket, prefix, key):\n",
    "#     create an in-memory binary stream\n",
    "    buf = io.BytesIO()\n",
    "#     to write the sample matrix and the label vector to that buffer in protobuf format\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, Y)\n",
    "    # use smac.write_numpy_to_dense_tensor(buf, feature, label) for numpy arrays\n",
    "    buf.seek(0)\n",
    "    print(buf)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "#     use boto3 to upload buffer to s3\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)\n",
    "    \n",
    "train_data = writeDatasetToProtobuf(X_train, Y_train, bucket, train_prefix, train_key)    \n",
    "test_data  = writeDatasetToProtobuf(X_test, Y_test, bucket, test_prefix, test_key)    \n",
    "  \n",
    "print(train_data)\n",
    "print(test_data)\n",
    "print('Output: {}'.format(output_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config and Run training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import image_uris\n",
    "\n",
    "region = boto3.Session().region_name    \n",
    "container = image_uris.retrieve('factorization-machines', region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-19 11:48:18 Starting - Starting the training job...\n",
      "2021-06-19 11:48:23 Starting - Launching requested ML instancesProfilerReport-1624103297: InProgress\n",
      "......\n",
      "2021-06-19 11:49:47 Starting - Preparing the instances for training......\n",
      "2021-06-19 11:50:46 Downloading - Downloading input data\n",
      "2021-06-19 11:50:46 Training - Downloading the training image...\n",
      "2021-06-19 11:51:16 Uploading - Uploading generated training model\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:87: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:120: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-conf.json: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0'}\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '2625', 'predictor_type': 'regressor', 'num_factors': '64', 'epochs': '10'}\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] Final configuration: {'epochs': '10', 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0', 'feature_dim': '2625', 'predictor_type': 'regressor', 'num_factors': '64'}\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 WARNING 139807005935424] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] Using default worker.\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2021-06-19 11:51:08.284] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2021-06-19 11:51:08.288] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 7, \"num_examples\": 1, \"num_bytes\": 64000}\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] nvidia-smi: took 0.029 seconds to run.\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103468.2811933, \"EndTime\": 1624103468.3216646, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 34.74998474121094, \"count\": 1, \"min\": 34.74998474121094, \"max\": 34.74998474121094}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103468.3217688, \"EndTime\": 1624103468.3218021, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[11:51:08] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.204642.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[11:51:08] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.204642.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=3.6775918262668847\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=13.524681640625\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=3.512104248046875\u001b[0m\n",
      "\u001b[34m[2021-06-19 11:51:08.885] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 543, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=1.7459121372381625\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] #quality_metric: host=algo-1, epoch=0, train mse <loss>=3.0482091909555287\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=1.3847017265571342\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103468.3217287, \"EndTime\": 1624103468.8860528, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"update.time\": {\"sum\": 564.0521049499512, \"count\": 1, \"min\": 564.0521049499512, \"max\": 564.0521049499512}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103468.3219736, \"EndTime\": 1624103468.886296, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 91570.0, \"count\": 1, \"min\": 91570, \"max\": 91570}, \"Total Batches Seen\": {\"sum\": 92.0, \"count\": 1, \"min\": 92, \"max\": 92}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] #throughput_metric: host=algo-1, train throughput=160466.29286299934 records/second\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=1.0820325473818937\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=1.17079443359375\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:08 INFO 139807005935424] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=0.9084769897460937\u001b[0m\n",
      "\u001b[34m[2021-06-19 11:51:09.462] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 573, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:09 INFO 139807005935424] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=1.1315323538388855\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:09 INFO 139807005935424] #quality_metric: host=algo-1, epoch=1, train mse <loss>=1.280365467784169\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:09 INFO 139807005935424] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=0.9466290873433207\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103468.8861322, \"EndTime\": 1624103469.4626467, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 575.8121013641357, \"count\": 1, \"min\": 575.8121013641357, \"max\": 575.8121013641357}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:09 INFO 139807005935424] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103468.8868098, \"EndTime\": 1624103469.4628587, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 182140.0, \"count\": 1, \"min\": 182140, \"max\": 182140}, \"Total Batches Seen\": {\"sum\": 183.0, \"count\": 1, \"min\": 183, \"max\": 183}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:09 INFO 139807005935424] #throughput_metric: host=algo-1, train throughput=157195.73518585876 records/second\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:09 INFO 139807005935424] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=1.065686398524444\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:09 INFO 139807005935424] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=1.1356875\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:09 INFO 139807005935424] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=0.8951871337890625\u001b[0m\n",
      "\u001b[34m[2021-06-19 11:51:10.081] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 616, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:10 INFO 139807005935424] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=1.113712291241248\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:10 INFO 139807005935424] #quality_metric: host=algo-1, epoch=2, train mse <loss>=1.2403550676618305\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:10 INFO 139807005935424] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=0.9298199261675825\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103469.462714, \"EndTime\": 1624103470.0823479, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 618.9050674438477, \"count\": 1, \"min\": 618.9050674438477, \"max\": 618.9050674438477}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:10 INFO 139807005935424] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103469.4634187, \"EndTime\": 1624103470.082607, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 272710.0, \"count\": 1, \"min\": 272710, \"max\": 272710}, \"Total Batches Seen\": {\"sum\": 274.0, \"count\": 1, \"min\": 274, \"max\": 274}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:10 INFO 139807005935424] #throughput_metric: host=algo-1, train throughput=146248.20866613873 records/second\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:10 INFO 139807005935424] #quality_metric: host=algo-1, epoch=3, batch=0 train rmse <loss>=1.0472716133034687\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:10 INFO 139807005935424] #quality_metric: host=algo-1, epoch=3, batch=0 train mse <loss>=1.09677783203125\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:10 INFO 139807005935424] #quality_metric: host=algo-1, epoch=3, batch=0 train absolute_loss <loss>=0.87847314453125\u001b[0m\n",
      "\u001b[34m[2021-06-19 11:51:10.675] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 591, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:10 INFO 139807005935424] #quality_metric: host=algo-1, epoch=3, train rmse <loss>=1.0947604829869821\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:10 INFO 139807005935424] #quality_metric: host=algo-1, epoch=3, train mse <loss>=1.1985005151098902\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:10 INFO 139807005935424] #quality_metric: host=algo-1, epoch=3, train absolute_loss <loss>=0.9109852308336195\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103470.0824032, \"EndTime\": 1624103470.675968, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 592.7379131317139, \"count\": 1, \"min\": 592.7379131317139, \"max\": 592.7379131317139}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:10 INFO 139807005935424] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103470.0832057, \"EndTime\": 1624103470.67618, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 363280.0, \"count\": 1, \"min\": 363280, \"max\": 363280}, \"Total Batches Seen\": {\"sum\": 365.0, \"count\": 1, \"min\": 365, \"max\": 365}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:10 INFO 139807005935424] #throughput_metric: host=algo-1, train throughput=152710.76758566406 records/second\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:10 INFO 139807005935424] #quality_metric: host=algo-1, epoch=4, batch=0 train rmse <loss>=1.0282144005583782\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:10 INFO 139807005935424] #quality_metric: host=algo-1, epoch=4, batch=0 train mse <loss>=1.057224853515625\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:10 INFO 139807005935424] #quality_metric: host=algo-1, epoch=4, batch=0 train absolute_loss <loss>=0.8601881103515625\u001b[0m\n",
      "\u001b[34m[2021-06-19 11:51:11.206] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 528, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:11 INFO 139807005935424] #quality_metric: host=algo-1, epoch=4, train rmse <loss>=1.0756978955438483\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:11 INFO 139807005935424] #quality_metric: host=algo-1, epoch=4, train mse <loss>=1.157125962477464\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:11 INFO 139807005935424] #quality_metric: host=algo-1, epoch=4, train absolute_loss <loss>=0.8909232077126975\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103470.6760364, \"EndTime\": 1624103471.207374, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 530.7178497314453, \"count\": 1, \"min\": 530.7178497314453, \"max\": 530.7178497314453}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:11 INFO 139807005935424] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103470.6766338, \"EndTime\": 1624103471.2075703, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 453850.0, \"count\": 1, \"min\": 453850, \"max\": 453850}, \"Total Batches Seen\": {\"sum\": 456.0, \"count\": 1, \"min\": 456, \"max\": 456}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:11 INFO 139807005935424] #throughput_metric: host=algo-1, train throughput=170552.36887049125 records/second\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:11 INFO 139807005935424] #quality_metric: host=algo-1, epoch=5, batch=0 train rmse <loss>=1.009796496747612\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:11 INFO 139807005935424] #quality_metric: host=algo-1, epoch=5, batch=0 train mse <loss>=1.01968896484375\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:11 INFO 139807005935424] #quality_metric: host=algo-1, epoch=5, batch=0 train absolute_loss <loss>=0.8415313110351562\u001b[0m\n",
      "\u001b[34m[2021-06-19 11:51:11.826] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 617, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:11 INFO 139807005935424] #quality_metric: host=algo-1, epoch=5, train rmse <loss>=1.057580237549661\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:11 INFO 139807005935424] #quality_metric: host=algo-1, epoch=5, train mse <loss>=1.1184759588555975\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:11 INFO 139807005935424] #quality_metric: host=algo-1, epoch=5, train absolute_loss <loss>=0.8707201430771377\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103471.2074332, \"EndTime\": 1624103471.8273368, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 619.1849708557129, \"count\": 1, \"min\": 619.1849708557129, \"max\": 619.1849708557129}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:11 INFO 139807005935424] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103471.2081273, \"EndTime\": 1624103471.8275309, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 544420.0, \"count\": 1, \"min\": 544420, \"max\": 544420}, \"Total Batches Seen\": {\"sum\": 547.0, \"count\": 1, \"min\": 547, \"max\": 547}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 7.0, \"count\": 1, \"min\": 7, \"max\": 7}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:11 INFO 139807005935424] #throughput_metric: host=algo-1, train throughput=146199.85978783443 records/second\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:11 INFO 139807005935424] #quality_metric: host=algo-1, epoch=6, batch=0 train rmse <loss>=0.9927840234348556\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:11 INFO 139807005935424] #quality_metric: host=algo-1, epoch=6, batch=0 train mse <loss>=0.9856201171875\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:11 INFO 139807005935424] #quality_metric: host=algo-1, epoch=6, batch=0 train absolute_loss <loss>=0.8234747924804687\u001b[0m\n",
      "\u001b[34m[2021-06-19 11:51:12.416] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 587, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:12 INFO 139807005935424] #quality_metric: host=algo-1, epoch=6, train rmse <loss>=1.0410052378651637\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:12 INFO 139807005935424] #quality_metric: host=algo-1, epoch=6, train mse <loss>=1.083691905262706\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:12 INFO 139807005935424] #quality_metric: host=algo-1, epoch=6, train absolute_loss <loss>=0.8513862512609461\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103471.827394, \"EndTime\": 1624103472.4166622, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 588.6762142181396, \"count\": 1, \"min\": 588.6762142181396, \"max\": 588.6762142181396}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:12 INFO 139807005935424] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103471.8279622, \"EndTime\": 1624103472.416888, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 634990.0, \"count\": 1, \"min\": 634990, \"max\": 634990}, \"Total Batches Seen\": {\"sum\": 638.0, \"count\": 1, \"min\": 638, \"max\": 638}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:12 INFO 139807005935424] #throughput_metric: host=algo-1, train throughput=153752.79759194434 records/second\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:12 INFO 139807005935424] #quality_metric: host=algo-1, epoch=7, batch=0 train rmse <loss>=0.977516908621342\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:12 INFO 139807005935424] #quality_metric: host=algo-1, epoch=7, batch=0 train mse <loss>=0.955539306640625\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:12 INFO 139807005935424] #quality_metric: host=algo-1, epoch=7, batch=0 train absolute_loss <loss>=0.8062623291015625\u001b[0m\n",
      "\u001b[34m[2021-06-19 11:51:12.967] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 548, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:12 INFO 139807005935424] #quality_metric: host=algo-1, epoch=7, train rmse <loss>=1.0262394918116269\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:12 INFO 139807005935424] #quality_metric: host=algo-1, epoch=7, train mse <loss>=1.053167494553786\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:12 INFO 139807005935424] #quality_metric: host=algo-1, epoch=7, train absolute_loss <loss>=0.8339391506320828\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103472.4167354, \"EndTime\": 1624103472.9680908, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 550.5900382995605, \"count\": 1, \"min\": 550.5900382995605, \"max\": 550.5900382995605}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:12 INFO 139807005935424] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103472.4174788, \"EndTime\": 1624103472.9682436, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 725560.0, \"count\": 1, \"min\": 725560, \"max\": 725560}, \"Total Batches Seen\": {\"sum\": 729.0, \"count\": 1, \"min\": 729, \"max\": 729}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 9.0, \"count\": 1, \"min\": 9, \"max\": 9}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:12 INFO 139807005935424] #throughput_metric: host=algo-1, train throughput=164420.43037438858 records/second\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:12 INFO 139807005935424] #quality_metric: host=algo-1, epoch=8, batch=0 train rmse <loss>=0.9640701288697343\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:12 INFO 139807005935424] #quality_metric: host=algo-1, epoch=8, batch=0 train mse <loss>=0.9294312133789062\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:12 INFO 139807005935424] #quality_metric: host=algo-1, epoch=8, batch=0 train absolute_loss <loss>=0.7909054565429687\u001b[0m\n",
      "\u001b[34m[2021-06-19 11:51:13.513] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 543, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:13 INFO 139807005935424] #quality_metric: host=algo-1, epoch=8, train rmse <loss>=1.0133308765976563\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:13 INFO 139807005935424] #quality_metric: host=algo-1, epoch=8, train mse <loss>=1.0268394654661746\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:13 INFO 139807005935424] #quality_metric: host=algo-1, epoch=8, train absolute_loss <loss>=0.8189971085433122\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103472.968146, \"EndTime\": 1624103473.5135531, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 544.851541519165, \"count\": 1, \"min\": 544.851541519165, \"max\": 544.851541519165}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:13 INFO 139807005935424] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103472.9686773, \"EndTime\": 1624103473.5137029, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 816130.0, \"count\": 1, \"min\": 816130, \"max\": 816130}, \"Total Batches Seen\": {\"sum\": 820.0, \"count\": 1, \"min\": 820, \"max\": 820}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:13 INFO 139807005935424] #throughput_metric: host=algo-1, train throughput=166150.68197434596 records/second\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:13 INFO 139807005935424] #quality_metric: host=algo-1, epoch=9, batch=0 train rmse <loss>=0.9523707312625819\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:13 INFO 139807005935424] #quality_metric: host=algo-1, epoch=9, batch=0 train mse <loss>=0.907010009765625\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:13 INFO 139807005935424] #quality_metric: host=algo-1, epoch=9, batch=0 train absolute_loss <loss>=0.7780084228515625\u001b[0m\n",
      "\u001b[34m[2021-06-19 11:51:14.049] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 534, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:14 INFO 139807005935424] #quality_metric: host=algo-1, epoch=9, train rmse <loss>=1.0021892218501025\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:14 INFO 139807005935424] #quality_metric: host=algo-1, epoch=9, train mse <loss>=1.0043832363925138\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:14 INFO 139807005935424] #quality_metric: host=algo-1, epoch=9, train absolute_loss <loss>=0.8065948251577524\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:14 INFO 139807005935424] #quality_metric: host=algo-1, train rmse <loss>=1.0021892218501025\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:14 INFO 139807005935424] #quality_metric: host=algo-1, train mse <loss>=1.0043832363925138\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:14 INFO 139807005935424] #quality_metric: host=algo-1, train absolute_loss <loss>=0.8065948251577524\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103473.513609, \"EndTime\": 1624103474.0504982, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 536.3156795501709, \"count\": 1, \"min\": 536.3156795501709, \"max\": 536.3156795501709}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:14 INFO 139807005935424] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103473.5141573, \"EndTime\": 1624103474.0507061, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 906700.0, \"count\": 1, \"min\": 906700, \"max\": 906700}, \"Total Batches Seen\": {\"sum\": 911.0, \"count\": 1, \"min\": 911, \"max\": 911}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 11.0, \"count\": 1, \"min\": 11, \"max\": 11}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:14 INFO 139807005935424] #throughput_metric: host=algo-1, train throughput=168765.93551872415 records/second\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:14 WARNING 139807005935424] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:14 INFO 139807005935424] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103474.0505662, \"EndTime\": 1624103474.0529199, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 1.9414424896240234, \"count\": 1, \"min\": 1.9414424896240234, \"max\": 1.9414424896240234}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:14 INFO 139807005935424] Saved checkpoint to \"/tmp/tmph4n18bm7/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2021-06-19 11:51:14.058] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 5773, \"num_examples\": 1, \"num_bytes\": 64000}\u001b[0m\n",
      "\u001b[34m[2021-06-19 11:51:14.082] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 23, \"num_examples\": 10, \"num_bytes\": 603520}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103474.0585644, \"EndTime\": 1624103474.0822046, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 9430.0, \"count\": 1, \"min\": 9430, \"max\": 9430}, \"Total Batches Seen\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Max Records Seen Between Resets\": {\"sum\": 9430.0, \"count\": 1, \"min\": 9430, \"max\": 9430}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 9430.0, \"count\": 1, \"min\": 9430, \"max\": 9430}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:14 INFO 139807005935424] #test_score (algo-1) : ('rmse', 1.0189945312319715)\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:14 INFO 139807005935424] #test_score (algo-1) : ('mse', 1.038349854680665)\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:14 INFO 139807005935424] #test_score (algo-1) : ('absolute_loss', 0.8378972948545615)\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:14 INFO 139807005935424] #quality_metric: host=algo-1, test rmse <loss>=1.0189945312319715\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:14 INFO 139807005935424] #quality_metric: host=algo-1, test mse <loss>=1.038349854680665\u001b[0m\n",
      "\u001b[34m[06/19/2021 11:51:14 INFO 139807005935424] #quality_metric: host=algo-1, test absolute_loss <loss>=0.8378972948545615\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1624103474.0529912, \"EndTime\": 1624103474.0828478, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 15.353679656982422, \"count\": 1, \"min\": 15.353679656982422, \"max\": 15.353679656982422}, \"totaltime\": {\"sum\": 5821.956157684326, \"count\": 1, \"min\": 5821.956157684326, \"max\": 5821.956157684326}}}\n",
      "\u001b[0m\n",
      "\n",
      "2021-06-19 11:51:47 Completed - Training job completed\n",
      "Training seconds: 49\n",
      "Billable seconds: 49\n"
     ]
    }
   ],
   "source": [
    "fm = sagemaker.estimator.Estimator(container,\n",
    "                                   role=sagemaker.get_execution_role(),\n",
    "                                   instance_count=1, \n",
    "                                   instance_type='ml.c5.xlarge',\n",
    "                                   output_path=output_prefix\n",
    "                                   )\n",
    "\n",
    "fm.set_hyperparameters(feature_dim=num_features,\n",
    "                      predictor_type='regressor',\n",
    "                      num_factors=64,\n",
    "                      epochs=10)\n",
    "# As protobuf is the default format for Factorization Machines,no need for training input.fm-estimator take protbuf paths.while other traiinginput channel\n",
    "fm.fit({'train': train_data, 'test': test_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'fm-movielens-100k'\n",
    "fm_predictor = fm.deploy(endpoint_name=endpoint_name,\n",
    "                         instance_type='ml.t2.medium', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send samples to the endpoint in JSON format\n",
    "import json\n",
    "# Serialization is converting an object into a stream of byte for action\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "class FMSerializer(JSONSerializer):\n",
    "    def serialize(self, data):\n",
    "       js = {'instances': []}\n",
    "       for row in data:\n",
    "              js['instances'].append({'features': row.tolist()})\n",
    "       return json.dumps(js)\n",
    "\n",
    "fm_predictor.serializer = FMSerializer()\n",
    "# The default JSON deserializer will be used automatically since we set the content type to application/json\n",
    "fm_predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'score': 3.3874545097351074}, {'score': 3.429487943649292}, {'score': 3.6385748386383057}]}\n"
     ]
    }
   ],
   "source": [
    "result = fm_predictor.predict(X_test[:3].toarray())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-1:742091327244:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
